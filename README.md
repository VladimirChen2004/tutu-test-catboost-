# tutu-test-catboost-

### Обзор

Задача: прогноз item_losses по заказам маркетплейса с временной валидацией и поддержкой большого числа категориальных и числовых признаков на GPU с CatBoost.
Реализованы два контура: базовая одноэтапная регрессия CatBoost и двухстадийная «hurdle»‑модель с калибровкой вероятностей и мягкой склейкой, а также опциональный блендинг с прямой регрессией.

### Подготовка данных

- Исключены служебные идентификаторы, сырые тексты и сырые datetime‑колонки, чтобы не передавать неподдерживаемые типы напрямую в модель.
- Числовые пропуски оставлены как np.nan для обработки CatBoost, а категориальные пропуски заменены служебным токеном, что соответствует нативной поддержке пропусков в числовых фичах и требованию строкового представления для категорий.
- Нормализация типов перед CatBoost: числовые приведены к float32, булевы к float, категориальные к строкам, что предотвращает ошибки сериализации и соответствует ожиданиям пула признаков.


### Валидация

Используется TimeSeriesSplit с сортировкой по order_date, чтобы сохранять причинность и избегать утечек информации из будущего в прошлое при OOF‑оценке

### Базовый baseline (одностадийный)

- Модель: CatBoostRegressor с RMSE, обучение на GPU с task_type="GPU", Poisson‑bootstrap и subsample, без colsample_bylevel на GPU, так как rsm/colsample_bylevel поддержан только для pairwise задач ранжирования.
- Параметры: depth, learning_rate, n_estimators с ранней остановкой по валидации, что согласуется с рекомендациями по ускорению/тюнингу в CatBoost.
- Метрика: OOF RMSE, рассчитывается только по валидационным индексам, так как тренировочные позиции в OOF остаются NaN по определению.


### Двухстадийная «hurdle»‑модель

- Стадия 1: CatBoostClassifier для вероятности y>0, обучение на GPU, с последующей калибровкой вероятностей без утечек на временном holdout внутри каждого train‑фолда (isotonic/Platt), что улучшает согласованность p и снижает ошибку при пороговых решениях.
- Стадия 2: CatBoostRegressor на подмножестве y>0, обучение на GPU с теми же безопасными параметрами, что и baseline.
- Склейка: мягкая формула ŷ = p^α · reg с подбором τ и α по OOF, где τ задаёт порог обнуления, а α — степень сглаживания вероятностей, что уменьшает шум на «почти положительных» примерах.
- Блендинг: линейная смесь с baseline‑регрессией по весу w, подбираемому на OOF, что повышает устойчивость и уменьшает риск деградации на пограничных вероятностях.


### Тюнинг

Рекомендуется подбирать depth, learning_rate, l2_leaf_reg и количество итераций под раннюю остановку на фиксированном TimeSeriesSplit, что соответствует руководству по параметрам и практикам ускорения CatBoost.

### Сохранение артефактов

- Модели CatBoost сохраняются в формате .cbm через save_model и восстанавливаются через load_model без повторного обучения, что является нативным способом переноса на прод.
- Калибратор вероятностей (IsotonicRegression/Platt) сериализуется через joblib, как стандартный механизм устойчивого сохранения объектов scikit‑learn.
- Метаданные инференса (feature_cols, cat_cols, tau_star, alpha_star, blend_weight, calib_kind) сохраняются в JSON, чтобы при загрузке воспроизвести точную схему признаков и логику склейки.


### Инференс

- Повторная подготовка фич: строго те же build_X → normalize_for_catboost и порядок колонок, что и при обучении, чтобы соответствовать сохранённым cat_features и распределению входов.
- Пайплайн: p_raw → калибратор → p_calibrated, затем регрессия и мягкая склейка с параметрами τ и α, после чего опциональный блендинг с baseline‑регрессией по весу w, получая итоговый item_losses.
- Сохранение результатов: финальный df_test с колонкой item_losses сохраняется через to_csv(index=False) в рабочую директорию для выгрузки из окружения.


### Что сохранено в коде

- «Первый CatBoost»: одноэтапная регрессия на GPU (baseline), служащая опорной моделью и компонентом блендинга в двухстадийной схеме.
- «Последний»: двухстадийная калиброванная модель с мягкой склейкой и блендингом, ориентированная на zero‑inflated распределение и снижение RMSE относительно baseline на тех же временных сплитах.


### Примечания

- На GPU не используйте colsample_bylevel/rsm в регрессии: на видеокартах это разрешено только для pairwise режимов ранжирования, иначе возникнут ошибки совместимости.
- Для пропусков: числовые оставляйте как np.nan (CatBoost обработает), а категориальные заполняйте токеном, чтобы пул корректно сериализовал входы и не возникало ошибок типов.
- Для стабильности RMSE контролируйте причинность и не смешивайте статистики из валидации при калибровке/импутации, используя временные holdout‑секции внутри train‑фолдов.


